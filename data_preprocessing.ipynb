{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "import time\n",
    "import collections\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentencePiece Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_enc = spm.SentencePieceProcessor()\n",
    "sp_enc.Load(\"sentencepiece_model/enc.model\")\n",
    "\n",
    "sp_dec = spm.SentencePieceProcessor()\n",
    "sp_dec.Load(\"sentencepiece_model/dec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.96 minutes to process train.en\n",
      "8.64 minutes to process train.de\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "\n",
    "train_enc = []\n",
    "with open(\"./data/train.en\", \"r\") as f:\n",
    "    for line in f.readlines() :\n",
    "        train_enc.append(sp_enc.EncodeAsIds(line))\n",
    "\n",
    "print(\"%.2f minutes to process train.en\" % ((time.time() - t) / 60))\n",
    "t = time.time()\n",
    "        \n",
    "train_dec = []\n",
    "with open(\"./data/train.de\", \"r\") as f:\n",
    "    for line in f.readlines() :\n",
    "        train_dec.append(sp_dec.EncodeAsIds(line))\n",
    "        \n",
    "print(\"%.2f minutes to process train.de\" % ((time.time() - t) / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFRecord-Dataset Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_int_feature(values):\n",
    "    feature = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 percent proceeded\n",
      "20 percent proceeded\n",
      "30 percent proceeded\n",
      "40 percent proceeded\n",
      "50 percent proceeded\n",
      "60 percent proceeded\n",
      "70 percent proceeded\n",
      "80 percent proceeded\n",
      "89 percent proceeded\n",
      "\n",
      "14.85 minutes to create TFRecord dataset\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "\n",
    "assert (len(train_enc) == len(train_dec))\n",
    "\n",
    "enc_max_len = 128\n",
    "dec_max_len = 128\n",
    "\n",
    "writer = []\n",
    "\n",
    "for f_num in range(0, 4) :\n",
    "    writer.append(tf.python_io.TFRecordWriter('./data/wmt2014_' + str(f_num) + '.tfrecord'))\n",
    "    \n",
    "proceed_rate = 0.1\n",
    "for i in range(len(train_enc)) :\n",
    "    if i>0 and float(i) / float(len(train_enc)) > proceed_rate :\n",
    "        print(\"%d percent proceeded\" % int(100*proceed_rate))\n",
    "        proceed_rate += 0.1\n",
    "\n",
    "    if len(train_enc[i]) > enc_max_len :\n",
    "        enc_input_id = train_enc[i][0:enc_max_len]\n",
    "        enc_labels = [1] * enc_max_len\n",
    "    else :\n",
    "        enc_input_id = train_enc[i] + [0] * (enc_max_len - len(train_enc[i]))\n",
    "        enc_labels = [1] * len(train_enc[i]) + [0] * (enc_max_len - len(train_enc[i]))\n",
    "\n",
    "    if len(train_dec[i]) > dec_max_len :\n",
    "        dec_input_id = train_dec[i][0:dec_max_len]\n",
    "        dec_labels = [1] * dec_max_len\n",
    "    else :\n",
    "        dec_input_id = train_dec[i] + [0] * (dec_max_len - len(train_dec[i]))\n",
    "        dec_labels = [1] * len(train_dec[i]) + [0] * (dec_max_len - len(train_dec[i]))\n",
    "\n",
    "    features = collections.OrderedDict()\n",
    "\n",
    "    features['enc_input_id'] = create_int_feature(enc_input_id)\n",
    "    features['enc_input_mask'] = create_int_feature(enc_labels)\n",
    "    features['dec_input_id'] = create_int_feature(dec_input_id)\n",
    "    features['dec_input_mask'] = create_int_feature(dec_labels)\n",
    "\n",
    "    example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "\n",
    "    writer[i%len(writer)].write(example.SerializeToString())\n",
    "\n",
    "for f_num in range(0, 4) :\n",
    "    writer[f_num].close()\n",
    "\n",
    "print(\"\\n%.2f minutes to create TFRecord dataset\" % ((time.time() - t) / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset load Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_features = {\n",
    "    'dec_input_id' : tf.FixedLenFeature([dec_max_len], tf.int64),\n",
    "    'dec_input_mask' : tf.FixedLenFeature([dec_max_len], tf.int64),\n",
    "    'enc_input_id' : tf.FixedLenFeature([enc_max_len], tf.int64),\n",
    "    'enc_input_mask' : tf.FixedLenFeature([enc_max_len], tf.int64)\n",
    "}\n",
    "\n",
    "def _decode_record(record, name_to_features) :\n",
    "    example = tf.parse_single_example(record, name_to_features)\n",
    "    \n",
    "    return example\n",
    "\n",
    "filenames = []\n",
    "for f_num in range(0, 4) :\n",
    "    filenames.append('./data/wmt2014_' + str(f_num) + '.tfrecord')\n",
    "\n",
    "d = tf.data.TFRecordDataset(filenames)\n",
    "\n",
    "d = d.apply(tf.contrib.data.map_and_batch(\n",
    "    lambda record : _decode_record(record, name_to_features),\n",
    "    batch_size=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apart from being Hungary ’ s principal political , commercial , industrial and transportation centre , the city of Budapest boast s sites , monuments and spas of worldwide renown . \n",
      "Budapest ist nicht nur das politische , wirtschaftliche , industrielle und verkehrs technische Herz Ungarns , sondern rühmt sich auch weltweit bekannter Sehenswürdigkei ten , Denkmäler und Bäder . \n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "select = 25\n",
    "\n",
    "for info in d :\n",
    "    cnt += 1\n",
    "    if cnt < select :\n",
    "        continue\n",
    "        \n",
    "    length = np.sum(info['enc_input_mask'][0])\n",
    "    tok = [sp_enc.IdToPiece(int(token)).replace(\"\\xe2\\x96\\x81\", \"\") for token in info['enc_input_id'][0][0:length]]\n",
    "    \n",
    "    out = \"\"\n",
    "    for token in tok :\n",
    "        out += \"%s \" % token\n",
    "        \n",
    "    print(out)\n",
    "    \n",
    "    length = np.sum(info['dec_input_mask'][0])\n",
    "    tok = [sp_dec.IdToPiece(int(token)).replace(\"\\xe2\\x96\\x81\", \"\") for token in info['dec_input_id'][0][0:length]]\n",
    "    \n",
    "    out = \"\"\n",
    "    for token in tok :\n",
    "        out += \"%s \" % token\n",
    "        \n",
    "    print(out)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py27-tf1.12.0",
   "language": "python",
   "name": "py27-tf1.12.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
